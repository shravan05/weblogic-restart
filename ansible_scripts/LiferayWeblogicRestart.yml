---
# Ansible script to do clean restart weblogic on target servers.
#Usage: ansible-playbook -i inventory Liferayweblogicstatus.yml -l <envType>
#Usage: ansible-playbook -i inventory Liferayweblogucstatus.yml -l DEV
##power devops-

- hosts: all
  remote_user: '{{ mi_user }}'
  become: yes
  become_user: oracle
  become_method: sudo

  vars_files:
    - ./vars/passwd.yml

  tasks:

  - name:  Stopping Managed Servers on node1
    shell: cd {{domainDir}}/bin && sh stopManagedWebLogic.sh {{ item }}
    with_items:
      - managed1
      - managed2
    ignore_errors: yes
    when: inventory_hostname in groups['liferayDEV01_Node1']
    run_once: true

  - name: Cleaning tmp & cache directories for Managed Server managed1
    file:
      path: '{{domainDir}}/servers/{{ item.servers }}/{{ item.deles }}'
      state: absent
    with_items:
      - { servers: 'managed1', deles: 'tmp' }
      - { servers: 'managed1', deles: 'cache' }
      - { servers: 'managed2', deles: 'tmp' }
      - { servers: 'managed2', deles: 'cache' }
    when: inventory_hostname in groups['liferayDEV01_Node1']
    run_once: true

  - name: Starting Managed Servers on Node1
    shell: cd {{domainDir}}/bin; nohup sh startManagedWebLogic.sh {{ item }} > {{ item }}.log &
    with_items:
      - managed1
      - managed2
    when: inventory_hostname in groups['liferayDEV01_Node1']
    run_once: true


  - name: Wait until managed server managed1 comes to RUNNING Mode
    wait_for:
      path: '{{domainDir}}/bin/{{ item }}.log'
      search_regex: RUNNING
    with_items:
      - managed1
      - managed2
    when: inventory_hostname in groups['liferayDEV01_Node1']
    run_once: true

  - name: Executing wlst script for Weblogic status
    shell: cd {{wlsDir}}/bin && sh serverstatus.sh {{weblogicUser}} {{weblogicPass}}
    register: resCmd
    when: inventory_hostname in groups['liferayDEV01_Node1']
    run_once: true

  - debug: msg='Below is the serverstatus..{{resCmd}}'
